{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is a test about recomputation acceleration for KV cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input parameters\n",
    "model_dim = 2000\n",
    "seq_len = 1000\n",
    "batch_size = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate W_q, W_k, W_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_q = torch.randn(model_dim, model_dim).half()\n",
    "W_k = torch.randn(model_dim, model_dim).half()\n",
    "W_v = torch.randn(model_dim, model_dim).half()\n",
    "prefil_kcache = torch.randn(batch_size,seq_len, model_dim).half()\n",
    "prefil_vcache = torch.randn(batch_size,seq_len, model_dim).half()\n",
    "new_token = torch.randn(batch_size,1, model_dim).half()\n",
    "all_tokens = torch.randn(batch_size,seq_len, model_dim).half()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_q = W_q.to(\"cuda\")\n",
    "W_k = W_k.to(\"cuda\")\n",
    "W_v = W_v.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all compute no kv cache\n",
    "import time\n",
    "all_tokens = all_tokens.to(\"cuda\")\n",
    "start = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/scratch/haokang/anaconda3/envs/kh/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before torch.Size([1, 13, 768]) torch.Size([1, 13, 768]) torch.Size([1, 13, 768])\n",
      "after torch.Size([1, 12, 13, 64]) torch.Size([1, 12, 13, 64]) torch.Size([1, 12, 13, 64])\n",
      "attn_output torch.Size([1, 12, 13, 64]) attn_weights torch.Size([1, 12, 13, 13])\n",
      "before torch.Size([1, 1, 768]) torch.Size([1, 1, 768]) torch.Size([1, 1, 768])\n",
      "after torch.Size([1, 12, 1, 64]) torch.Size([1, 12, 14, 64]) torch.Size([1, 12, 14, 64])\n",
      "attn_output torch.Size([1, 12, 1, 64]) attn_weights torch.Size([1, 12, 1, 14])\n",
      "before torch.Size([1, 1, 768]) torch.Size([1, 1, 768]) torch.Size([1, 1, 768])\n",
      "after torch.Size([1, 12, 1, 64]) torch.Size([1, 12, 15, 64]) torch.Size([1, 12, 15, 64])\n",
      "attn_output torch.Size([1, 12, 1, 64]) attn_weights torch.Size([1, 12, 1, 15])\n",
      "before torch.Size([1, 1, 768]) torch.Size([1, 1, 768]) torch.Size([1, 1, 768])\n",
      "after torch.Size([1, 12, 1, 64]) torch.Size([1, 12, 16, 64]) torch.Size([1, 12, 16, 64])\n",
      "attn_output torch.Size([1, 12, 1, 64]) attn_weights torch.Size([1, 12, 1, 16])\n",
      "before torch.Size([1, 1, 768]) torch.Size([1, 1, 768]) torch.Size([1, 1, 768])\n",
      "after torch.Size([1, 12, 1, 64]) torch.Size([1, 12, 17, 64]) torch.Size([1, 12, 17, 64])\n",
      "attn_output torch.Size([1, 12, 1, 64]) attn_weights torch.Size([1, 12, 1, 17])\n",
      "before torch.Size([1, 1, 768]) torch.Size([1, 1, 768]) torch.Size([1, 1, 768])\n",
      "after torch.Size([1, 12, 1, 64]) torch.Size([1, 12, 18, 64]) torch.Size([1, 12, 18, 64])\n",
      "attn_output torch.Size([1, 12, 1, 64]) attn_weights torch.Size([1, 12, 1, 18])\n",
      "before torch.Size([1, 1, 768]) torch.Size([1, 1, 768]) torch.Size([1, 1, 768])\n",
      "after torch.Size([1, 12, 1, 64]) torch.Size([1, 12, 19, 64]) torch.Size([1, 12, 19, 64])\n",
      "attn_output torch.Size([1, 12, 1, 64]) attn_weights torch.Size([1, 12, 1, 19])\n",
      "before torch.Size([1, 1, 768]) torch.Size([1, 1, 768]) torch.Size([1, 1, 768])\n",
      "after torch.Size([1, 12, 1, 64]) torch.Size([1, 12, 20, 64]) torch.Size([1, 12, 20, 64])\n",
      "attn_output torch.Size([1, 12, 1, 64]) attn_weights torch.Size([1, 12, 1, 20])\n",
      "before torch.Size([1, 1, 768]) torch.Size([1, 1, 768]) torch.Size([1, 1, 768])\n",
      "after torch.Size([1, 12, 1, 64]) torch.Size([1, 12, 21, 64]) torch.Size([1, 12, 21, 64])\n",
      "attn_output torch.Size([1, 12, 1, 64]) attn_weights torch.Size([1, 12, 1, 21])\n",
      "before torch.Size([1, 1, 768]) torch.Size([1, 1, 768]) torch.Size([1, 1, 768])\n",
      "after torch.Size([1, 12, 1, 64]) torch.Size([1, 12, 22, 64]) torch.Size([1, 12, 22, 64])\n",
      "attn_output torch.Size([1, 12, 1, 64]) attn_weights torch.Size([1, 12, 1, 22])\n",
      "before torch.Size([1, 1, 768]) torch.Size([1, 1, 768]) torch.Size([1, 1, 768])\n",
      "after torch.Size([1, 12, 1, 64]) torch.Size([1, 12, 23, 64]) torch.Size([1, 12, 23, 64])\n",
      "attn_output torch.Size([1, 12, 1, 64]) attn_weights torch.Size([1, 12, 1, 23])\n",
      "before torch.Size([1, 1, 768]) torch.Size([1, 1, 768]) torch.Size([1, 1, 768])\n",
      "after torch.Size([1, 12, 1, 64]) torch.Size([1, 12, 24, 64]) torch.Size([1, 12, 24, 64])\n",
      "attn_output torch.Size([1, 12, 1, 64]) attn_weights torch.Size([1, 12, 1, 24])\n",
      "before torch.Size([1, 1, 768]) torch.Size([1, 1, 768]) torch.Size([1, 1, 768])\n",
      "after torch.Size([1, 12, 1, 64]) torch.Size([1, 12, 25, 64]) torch.Size([1, 12, 25, 64])\n",
      "attn_output torch.Size([1, 12, 1, 64]) attn_weights torch.Size([1, 12, 1, 25])\n",
      "before torch.Size([1, 1, 768]) torch.Size([1, 1, 768]) torch.Size([1, 1, 768])\n",
      "after torch.Size([1, 12, 1, 64]) torch.Size([1, 12, 26, 64]) torch.Size([1, 12, 26, 64])\n",
      "attn_output torch.Size([1, 12, 1, 64]) attn_weights torch.Size([1, 12, 1, 26])\n",
      "before torch.Size([1, 1, 768]) torch.Size([1, 1, 768]) torch.Size([1, 1, 768])\n",
      "after torch.Size([1, 12, 1, 64]) torch.Size([1, 12, 27, 64]) torch.Size([1, 12, 27, 64])\n",
      "attn_output torch.Size([1, 12, 1, 64]) attn_weights torch.Size([1, 12, 1, 27])\n",
      "before torch.Size([1, 1, 768]) torch.Size([1, 1, 768]) torch.Size([1, 1, 768])\n",
      "after torch.Size([1, 12, 1, 64]) torch.Size([1, 12, 28, 64]) torch.Size([1, 12, 28, 64])\n",
      "attn_output torch.Size([1, 12, 1, 64]) attn_weights torch.Size([1, 12, 1, 28])\n",
      "before torch.Size([1, 1, 768]) torch.Size([1, 1, 768]) torch.Size([1, 1, 768])\n",
      "after torch.Size([1, 12, 1, 64]) torch.Size([1, 12, 29, 64]) torch.Size([1, 12, 29, 64])\n",
      "attn_output torch.Size([1, 12, 1, 64]) attn_weights torch.Size([1, 12, 1, 29])\n",
      "before torch.Size([1, 1, 768]) torch.Size([1, 1, 768]) torch.Size([1, 1, 768])\n",
      "after torch.Size([1, 12, 1, 64]) torch.Size([1, 12, 30, 64]) torch.Size([1, 12, 30, 64])\n",
      "attn_output torch.Size([1, 12, 1, 64]) attn_weights torch.Size([1, 12, 1, 30])\n",
      "before torch.Size([1, 1, 768]) torch.Size([1, 1, 768]) torch.Size([1, 1, 768])\n",
      "after torch.Size([1, 12, 1, 64]) torch.Size([1, 12, 31, 64]) torch.Size([1, 12, 31, 64])\n",
      "attn_output torch.Size([1, 12, 1, 64]) attn_weights torch.Size([1, 12, 1, 31])\n",
      "before torch.Size([1, 1, 768]) torch.Size([1, 1, 768]) torch.Size([1, 1, 768])\n",
      "after torch.Size([1, 12, 1, 64]) torch.Size([1, 12, 32, 64]) torch.Size([1, 12, 32, 64])\n",
      "attn_output torch.Size([1, 12, 1, 64]) attn_weights torch.Size([1, 12, 1, 32])\n",
      "before torch.Size([1, 1, 768]) torch.Size([1, 1, 768]) torch.Size([1, 1, 768])\n",
      "after torch.Size([1, 12, 1, 64]) torch.Size([1, 12, 33, 64]) torch.Size([1, 12, 33, 64])\n",
      "attn_output torch.Size([1, 12, 1, 64]) attn_weights torch.Size([1, 12, 1, 33])\n",
      "before torch.Size([1, 1, 768]) torch.Size([1, 1, 768]) torch.Size([1, 1, 768])\n",
      "after torch.Size([1, 12, 1, 64]) torch.Size([1, 12, 34, 64]) torch.Size([1, 12, 34, 64])\n",
      "attn_output torch.Size([1, 12, 1, 64]) attn_weights torch.Size([1, 12, 1, 34])\n",
      "before torch.Size([1, 1, 768]) torch.Size([1, 1, 768]) torch.Size([1, 1, 768])\n",
      "after torch.Size([1, 12, 1, 64]) torch.Size([1, 12, 35, 64]) torch.Size([1, 12, 35, 64])\n",
      "attn_output torch.Size([1, 12, 1, 64]) attn_weights torch.Size([1, 12, 1, 35])\n",
      "before torch.Size([1, 1, 768]) torch.Size([1, 1, 768]) torch.Size([1, 1, 768])\n",
      "after torch.Size([1, 12, 1, 64]) torch.Size([1, 12, 36, 64]) torch.Size([1, 12, 36, 64])\n",
      "attn_output torch.Size([1, 12, 1, 64]) attn_weights torch.Size([1, 12, 1, 36])\n",
      "before torch.Size([1, 1, 768]) torch.Size([1, 1, 768]) torch.Size([1, 1, 768])\n",
      "after torch.Size([1, 12, 1, 64]) torch.Size([1, 12, 37, 64]) torch.Size([1, 12, 37, 64])\n",
      "attn_output torch.Size([1, 12, 1, 64]) attn_weights torch.Size([1, 12, 1, 37])\n",
      "before torch.Size([1, 1, 768]) torch.Size([1, 1, 768]) torch.Size([1, 1, 768])\n",
      "after torch.Size([1, 12, 1, 64]) torch.Size([1, 12, 38, 64]) torch.Size([1, 12, 38, 64])\n",
      "attn_output torch.Size([1, 12, 1, 64]) attn_weights torch.Size([1, 12, 1, 38])\n",
      "before torch.Size([1, 1, 768]) torch.Size([1, 1, 768]) torch.Size([1, 1, 768])\n",
      "after torch.Size([1, 12, 1, 64]) torch.Size([1, 12, 39, 64]) torch.Size([1, 12, 39, 64])\n",
      "attn_output torch.Size([1, 12, 1, 64]) attn_weights torch.Size([1, 12, 1, 39])\n",
      "before torch.Size([1, 1, 768]) torch.Size([1, 1, 768]) torch.Size([1, 1, 768])\n",
      "after torch.Size([1, 12, 1, 64]) torch.Size([1, 12, 40, 64]) torch.Size([1, 12, 40, 64])\n",
      "attn_output torch.Size([1, 12, 1, 64]) attn_weights torch.Size([1, 12, 1, 40])\n",
      "before torch.Size([1, 1, 768]) torch.Size([1, 1, 768]) torch.Size([1, 1, 768])\n",
      "after torch.Size([1, 12, 1, 64]) torch.Size([1, 12, 41, 64]) torch.Size([1, 12, 41, 64])\n",
      "attn_output torch.Size([1, 12, 1, 64]) attn_weights torch.Size([1, 12, 1, 41])\n",
      "before torch.Size([1, 1, 768]) torch.Size([1, 1, 768]) torch.Size([1, 1, 768])\n",
      "after torch.Size([1, 12, 1, 64]) torch.Size([1, 12, 42, 64]) torch.Size([1, 12, 42, 64])\n",
      "attn_output torch.Size([1, 12, 1, 64]) attn_weights torch.Size([1, 12, 1, 42])\n",
      "before torch.Size([1, 1, 768]) torch.Size([1, 1, 768]) torch.Size([1, 1, 768])\n",
      "after torch.Size([1, 12, 1, 64]) torch.Size([1, 12, 43, 64]) torch.Size([1, 12, 43, 64])\n",
      "attn_output torch.Size([1, 12, 1, 64]) attn_weights torch.Size([1, 12, 1, 43])\n",
      "before torch.Size([1, 1, 768]) torch.Size([1, 1, 768]) torch.Size([1, 1, 768])\n",
      "after torch.Size([1, 12, 1, 64]) torch.Size([1, 12, 44, 64]) torch.Size([1, 12, 44, 64])\n",
      "attn_output torch.Size([1, 12, 1, 64]) attn_weights torch.Size([1, 12, 1, 44])\n",
      "before torch.Size([1, 1, 768]) torch.Size([1, 1, 768]) torch.Size([1, 1, 768])\n",
      "after torch.Size([1, 12, 1, 64]) torch.Size([1, 12, 45, 64]) torch.Size([1, 12, 45, 64])\n",
      "attn_output torch.Size([1, 12, 1, 64]) attn_weights torch.Size([1, 12, 1, 45])\n",
      "before torch.Size([1, 1, 768]) torch.Size([1, 1, 768]) torch.Size([1, 1, 768])\n",
      "after torch.Size([1, 12, 1, 64]) torch.Size([1, 12, 46, 64]) torch.Size([1, 12, 46, 64])\n",
      "attn_output torch.Size([1, 12, 1, 64]) attn_weights torch.Size([1, 12, 1, 46])\n",
      "before torch.Size([1, 1, 768]) torch.Size([1, 1, 768]) torch.Size([1, 1, 768])\n",
      "after torch.Size([1, 12, 1, 64]) torch.Size([1, 12, 47, 64]) torch.Size([1, 12, 47, 64])\n",
      "attn_output torch.Size([1, 12, 1, 64]) attn_weights torch.Size([1, 12, 1, 47])\n",
      "before torch.Size([1, 1, 768]) torch.Size([1, 1, 768]) torch.Size([1, 1, 768])\n",
      "after torch.Size([1, 12, 1, 64]) torch.Size([1, 12, 48, 64]) torch.Size([1, 12, 48, 64])\n",
      "attn_output torch.Size([1, 12, 1, 64]) attn_weights torch.Size([1, 12, 1, 48])\n",
      "before torch.Size([1, 1, 768]) torch.Size([1, 1, 768]) torch.Size([1, 1, 768])\n",
      "after torch.Size([1, 12, 1, 64]) torch.Size([1, 12, 49, 64]) torch.Size([1, 12, 49, 64])\n",
      "attn_output torch.Size([1, 12, 1, 64]) attn_weights torch.Size([1, 12, 1, 49])\n",
      "Generated Sentence:  Once upon a time, in a land far, far away, the world was a place of great beauty and great danger. The world of the gods was the land of darkness and darkness. And the darkness of this world, which was far from the\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "# Load the pre-trained GPT-2 model and tokenizer\n",
    "model_name = \"gpt2\"  # You can specify the model size, e.g., \"gpt2-medium\" for a different model size\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name, use_cache=True) # disable cache\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name) \n",
    "\n",
    "# Define a prompt\n",
    "prompt = \"Once upon a time, in a land far, far away,\"\n",
    "\n",
    "# Generate a sentence based on the prompt\n",
    "input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "output = model.generate(input_ids, max_length=50, num_return_sequences=1, no_repeat_ngram_size=2, top_k=50, top_p=0.95)\n",
    "\n",
    "# Decode and print the generated sentence\n",
    "generated_sentence = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "print(\"Generated Sentence: \", generated_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/scratch/haokang/anaconda3/envs/kh/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before torch.Size([1, 13, 768]) torch.Size([1, 13, 768]) torch.Size([1, 13, 768])\n",
      "after torch.Size([1, 12, 13, 64]) torch.Size([1, 12, 13, 64]) torch.Size([1, 12, 13, 64])\n",
      "attn_output torch.Size([1, 12, 13, 64]) attn_weights torch.Size([1, 12, 13, 13])\n",
      "before torch.Size([1, 14, 768]) torch.Size([1, 14, 768]) torch.Size([1, 14, 768])\n",
      "after torch.Size([1, 12, 14, 64]) torch.Size([1, 12, 14, 64]) torch.Size([1, 12, 14, 64])\n",
      "attn_output torch.Size([1, 12, 14, 64]) attn_weights torch.Size([1, 12, 14, 14])\n",
      "before torch.Size([1, 15, 768]) torch.Size([1, 15, 768]) torch.Size([1, 15, 768])\n",
      "after torch.Size([1, 12, 15, 64]) torch.Size([1, 12, 15, 64]) torch.Size([1, 12, 15, 64])\n",
      "attn_output torch.Size([1, 12, 15, 64]) attn_weights torch.Size([1, 12, 15, 15])\n",
      "before torch.Size([1, 16, 768]) torch.Size([1, 16, 768]) torch.Size([1, 16, 768])\n",
      "after torch.Size([1, 12, 16, 64]) torch.Size([1, 12, 16, 64]) torch.Size([1, 12, 16, 64])\n",
      "attn_output torch.Size([1, 12, 16, 64]) attn_weights torch.Size([1, 12, 16, 16])\n",
      "before torch.Size([1, 17, 768]) torch.Size([1, 17, 768]) torch.Size([1, 17, 768])\n",
      "after torch.Size([1, 12, 17, 64]) torch.Size([1, 12, 17, 64]) torch.Size([1, 12, 17, 64])\n",
      "attn_output torch.Size([1, 12, 17, 64]) attn_weights torch.Size([1, 12, 17, 17])\n",
      "before torch.Size([1, 18, 768]) torch.Size([1, 18, 768]) torch.Size([1, 18, 768])\n",
      "after torch.Size([1, 12, 18, 64]) torch.Size([1, 12, 18, 64]) torch.Size([1, 12, 18, 64])\n",
      "attn_output torch.Size([1, 12, 18, 64]) attn_weights torch.Size([1, 12, 18, 18])\n",
      "before torch.Size([1, 19, 768]) torch.Size([1, 19, 768]) torch.Size([1, 19, 768])\n",
      "after torch.Size([1, 12, 19, 64]) torch.Size([1, 12, 19, 64]) torch.Size([1, 12, 19, 64])\n",
      "attn_output torch.Size([1, 12, 19, 64]) attn_weights torch.Size([1, 12, 19, 19])\n",
      "before torch.Size([1, 20, 768]) torch.Size([1, 20, 768]) torch.Size([1, 20, 768])\n",
      "after torch.Size([1, 12, 20, 64]) torch.Size([1, 12, 20, 64]) torch.Size([1, 12, 20, 64])\n",
      "attn_output torch.Size([1, 12, 20, 64]) attn_weights torch.Size([1, 12, 20, 20])\n",
      "before torch.Size([1, 21, 768]) torch.Size([1, 21, 768]) torch.Size([1, 21, 768])\n",
      "after torch.Size([1, 12, 21, 64]) torch.Size([1, 12, 21, 64]) torch.Size([1, 12, 21, 64])\n",
      "attn_output torch.Size([1, 12, 21, 64]) attn_weights torch.Size([1, 12, 21, 21])\n",
      "before torch.Size([1, 22, 768]) torch.Size([1, 22, 768]) torch.Size([1, 22, 768])\n",
      "after torch.Size([1, 12, 22, 64]) torch.Size([1, 12, 22, 64]) torch.Size([1, 12, 22, 64])\n",
      "attn_output torch.Size([1, 12, 22, 64]) attn_weights torch.Size([1, 12, 22, 22])\n",
      "before torch.Size([1, 23, 768]) torch.Size([1, 23, 768]) torch.Size([1, 23, 768])\n",
      "after torch.Size([1, 12, 23, 64]) torch.Size([1, 12, 23, 64]) torch.Size([1, 12, 23, 64])\n",
      "attn_output torch.Size([1, 12, 23, 64]) attn_weights torch.Size([1, 12, 23, 23])\n",
      "before torch.Size([1, 24, 768]) torch.Size([1, 24, 768]) torch.Size([1, 24, 768])\n",
      "after torch.Size([1, 12, 24, 64]) torch.Size([1, 12, 24, 64]) torch.Size([1, 12, 24, 64])\n",
      "attn_output torch.Size([1, 12, 24, 64]) attn_weights torch.Size([1, 12, 24, 24])\n",
      "before torch.Size([1, 25, 768]) torch.Size([1, 25, 768]) torch.Size([1, 25, 768])\n",
      "after torch.Size([1, 12, 25, 64]) torch.Size([1, 12, 25, 64]) torch.Size([1, 12, 25, 64])\n",
      "attn_output torch.Size([1, 12, 25, 64]) attn_weights torch.Size([1, 12, 25, 25])\n",
      "before torch.Size([1, 26, 768]) torch.Size([1, 26, 768]) torch.Size([1, 26, 768])\n",
      "after torch.Size([1, 12, 26, 64]) torch.Size([1, 12, 26, 64]) torch.Size([1, 12, 26, 64])\n",
      "attn_output torch.Size([1, 12, 26, 64]) attn_weights torch.Size([1, 12, 26, 26])\n",
      "before torch.Size([1, 27, 768]) torch.Size([1, 27, 768]) torch.Size([1, 27, 768])\n",
      "after torch.Size([1, 12, 27, 64]) torch.Size([1, 12, 27, 64]) torch.Size([1, 12, 27, 64])\n",
      "attn_output torch.Size([1, 12, 27, 64]) attn_weights torch.Size([1, 12, 27, 27])\n",
      "before torch.Size([1, 28, 768]) torch.Size([1, 28, 768]) torch.Size([1, 28, 768])\n",
      "after torch.Size([1, 12, 28, 64]) torch.Size([1, 12, 28, 64]) torch.Size([1, 12, 28, 64])\n",
      "attn_output torch.Size([1, 12, 28, 64]) attn_weights torch.Size([1, 12, 28, 28])\n",
      "before torch.Size([1, 29, 768]) torch.Size([1, 29, 768]) torch.Size([1, 29, 768])\n",
      "after torch.Size([1, 12, 29, 64]) torch.Size([1, 12, 29, 64]) torch.Size([1, 12, 29, 64])\n",
      "attn_output torch.Size([1, 12, 29, 64]) attn_weights torch.Size([1, 12, 29, 29])\n",
      "before torch.Size([1, 30, 768]) torch.Size([1, 30, 768]) torch.Size([1, 30, 768])\n",
      "after torch.Size([1, 12, 30, 64]) torch.Size([1, 12, 30, 64]) torch.Size([1, 12, 30, 64])\n",
      "attn_output torch.Size([1, 12, 30, 64]) attn_weights torch.Size([1, 12, 30, 30])\n",
      "before torch.Size([1, 31, 768]) torch.Size([1, 31, 768]) torch.Size([1, 31, 768])\n",
      "after torch.Size([1, 12, 31, 64]) torch.Size([1, 12, 31, 64]) torch.Size([1, 12, 31, 64])\n",
      "attn_output torch.Size([1, 12, 31, 64]) attn_weights torch.Size([1, 12, 31, 31])\n",
      "before torch.Size([1, 32, 768]) torch.Size([1, 32, 768]) torch.Size([1, 32, 768])\n",
      "after torch.Size([1, 12, 32, 64]) torch.Size([1, 12, 32, 64]) torch.Size([1, 12, 32, 64])\n",
      "attn_output torch.Size([1, 12, 32, 64]) attn_weights torch.Size([1, 12, 32, 32])\n",
      "before torch.Size([1, 33, 768]) torch.Size([1, 33, 768]) torch.Size([1, 33, 768])\n",
      "after torch.Size([1, 12, 33, 64]) torch.Size([1, 12, 33, 64]) torch.Size([1, 12, 33, 64])\n",
      "attn_output torch.Size([1, 12, 33, 64]) attn_weights torch.Size([1, 12, 33, 33])\n",
      "before torch.Size([1, 34, 768]) torch.Size([1, 34, 768]) torch.Size([1, 34, 768])\n",
      "after torch.Size([1, 12, 34, 64]) torch.Size([1, 12, 34, 64]) torch.Size([1, 12, 34, 64])\n",
      "attn_output torch.Size([1, 12, 34, 64]) attn_weights torch.Size([1, 12, 34, 34])\n",
      "before torch.Size([1, 35, 768]) torch.Size([1, 35, 768]) torch.Size([1, 35, 768])\n",
      "after torch.Size([1, 12, 35, 64]) torch.Size([1, 12, 35, 64]) torch.Size([1, 12, 35, 64])\n",
      "attn_output torch.Size([1, 12, 35, 64]) attn_weights torch.Size([1, 12, 35, 35])\n",
      "before torch.Size([1, 36, 768]) torch.Size([1, 36, 768]) torch.Size([1, 36, 768])\n",
      "after torch.Size([1, 12, 36, 64]) torch.Size([1, 12, 36, 64]) torch.Size([1, 12, 36, 64])\n",
      "attn_output torch.Size([1, 12, 36, 64]) attn_weights torch.Size([1, 12, 36, 36])\n",
      "before torch.Size([1, 37, 768]) torch.Size([1, 37, 768]) torch.Size([1, 37, 768])\n",
      "after torch.Size([1, 12, 37, 64]) torch.Size([1, 12, 37, 64]) torch.Size([1, 12, 37, 64])\n",
      "attn_output torch.Size([1, 12, 37, 64]) attn_weights torch.Size([1, 12, 37, 37])\n",
      "before torch.Size([1, 38, 768]) torch.Size([1, 38, 768]) torch.Size([1, 38, 768])\n",
      "after torch.Size([1, 12, 38, 64]) torch.Size([1, 12, 38, 64]) torch.Size([1, 12, 38, 64])\n",
      "attn_output torch.Size([1, 12, 38, 64]) attn_weights torch.Size([1, 12, 38, 38])\n",
      "before torch.Size([1, 39, 768]) torch.Size([1, 39, 768]) torch.Size([1, 39, 768])\n",
      "after torch.Size([1, 12, 39, 64]) torch.Size([1, 12, 39, 64]) torch.Size([1, 12, 39, 64])\n",
      "attn_output torch.Size([1, 12, 39, 64]) attn_weights torch.Size([1, 12, 39, 39])\n",
      "before torch.Size([1, 40, 768]) torch.Size([1, 40, 768]) torch.Size([1, 40, 768])\n",
      "after torch.Size([1, 12, 40, 64]) torch.Size([1, 12, 40, 64]) torch.Size([1, 12, 40, 64])\n",
      "attn_output torch.Size([1, 12, 40, 64]) attn_weights torch.Size([1, 12, 40, 40])\n",
      "before torch.Size([1, 41, 768]) torch.Size([1, 41, 768]) torch.Size([1, 41, 768])\n",
      "after torch.Size([1, 12, 41, 64]) torch.Size([1, 12, 41, 64]) torch.Size([1, 12, 41, 64])\n",
      "attn_output torch.Size([1, 12, 41, 64]) attn_weights torch.Size([1, 12, 41, 41])\n",
      "before torch.Size([1, 42, 768]) torch.Size([1, 42, 768]) torch.Size([1, 42, 768])\n",
      "after torch.Size([1, 12, 42, 64]) torch.Size([1, 12, 42, 64]) torch.Size([1, 12, 42, 64])\n",
      "attn_output torch.Size([1, 12, 42, 64]) attn_weights torch.Size([1, 12, 42, 42])\n",
      "before torch.Size([1, 43, 768]) torch.Size([1, 43, 768]) torch.Size([1, 43, 768])\n",
      "after torch.Size([1, 12, 43, 64]) torch.Size([1, 12, 43, 64]) torch.Size([1, 12, 43, 64])\n",
      "attn_output torch.Size([1, 12, 43, 64]) attn_weights torch.Size([1, 12, 43, 43])\n",
      "before torch.Size([1, 44, 768]) torch.Size([1, 44, 768]) torch.Size([1, 44, 768])\n",
      "after torch.Size([1, 12, 44, 64]) torch.Size([1, 12, 44, 64]) torch.Size([1, 12, 44, 64])\n",
      "attn_output torch.Size([1, 12, 44, 64]) attn_weights torch.Size([1, 12, 44, 44])\n",
      "before torch.Size([1, 45, 768]) torch.Size([1, 45, 768]) torch.Size([1, 45, 768])\n",
      "after torch.Size([1, 12, 45, 64]) torch.Size([1, 12, 45, 64]) torch.Size([1, 12, 45, 64])\n",
      "attn_output torch.Size([1, 12, 45, 64]) attn_weights torch.Size([1, 12, 45, 45])\n",
      "before torch.Size([1, 46, 768]) torch.Size([1, 46, 768]) torch.Size([1, 46, 768])\n",
      "after torch.Size([1, 12, 46, 64]) torch.Size([1, 12, 46, 64]) torch.Size([1, 12, 46, 64])\n",
      "attn_output torch.Size([1, 12, 46, 64]) attn_weights torch.Size([1, 12, 46, 46])\n",
      "before torch.Size([1, 47, 768]) torch.Size([1, 47, 768]) torch.Size([1, 47, 768])\n",
      "after torch.Size([1, 12, 47, 64]) torch.Size([1, 12, 47, 64]) torch.Size([1, 12, 47, 64])\n",
      "attn_output torch.Size([1, 12, 47, 64]) attn_weights torch.Size([1, 12, 47, 47])\n",
      "before torch.Size([1, 48, 768]) torch.Size([1, 48, 768]) torch.Size([1, 48, 768])\n",
      "after torch.Size([1, 12, 48, 64]) torch.Size([1, 12, 48, 64]) torch.Size([1, 12, 48, 64])\n",
      "attn_output torch.Size([1, 12, 48, 64]) attn_weights torch.Size([1, 12, 48, 48])\n",
      "before torch.Size([1, 49, 768]) torch.Size([1, 49, 768]) torch.Size([1, 49, 768])\n",
      "after torch.Size([1, 12, 49, 64]) torch.Size([1, 12, 49, 64]) torch.Size([1, 12, 49, 64])\n",
      "attn_output torch.Size([1, 12, 49, 64]) attn_weights torch.Size([1, 12, 49, 49])\n",
      "Generated Sentence:  Once upon a time, in a land far, far away, the world was a place of great beauty and great danger. The world of the gods was the land of darkness and darkness. And the darkness of this world, which was far from the\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "# Load the pre-trained GPT-2 model and tokenizer\n",
    "model_name = \"gpt2\"  # You can specify the model size, e.g., \"gpt2-medium\" for a different model size\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name, use_cache=False) # disable cache\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name) \n",
    "\n",
    "# Define a prompt\n",
    "prompt = \"Once upon a time, in a land far, far away,\"\n",
    "\n",
    "# Generate a sentence based on the prompt\n",
    "input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "output = model.generate(input_ids, max_length=50, num_return_sequences=1, no_repeat_ngram_size=2, top_k=50, top_p=0.95)\n",
    "\n",
    "# Decode and print the generated sentence\n",
    "generated_sentence = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "print(\"Generated Sentence: \", generated_sentence)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
