{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/scratch/haokang/anaconda3/envs/test/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from transformers import MistralForCausalLM,MistralConfig\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_ID = \"mistralai/Mistral-7B-v0.1\"\n",
    "model_kwargs = {}\n",
    "model_kwargs[\"torch_dtype\"] = torch.float16\n",
    "model_kwargs[\"device_map\"] = \"auto\"\n",
    "model_kwargs[\"token\"] = None\n",
    "model_kwargs[\"cache_dir\"] = \"../cache\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/scratch/haokang/anaconda3/envs/test/lib/python3.10/site-packages/transformers/configuration_utils.py:508: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.35s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = MistralConfig.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    use_auth_token=True,\n",
    "    token=None,\n",
    "    use_flash_attn=False,\n",
    ")\n",
    "model = MistralForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    config=config,\n",
    "    **model_kwargs,\n",
    "    # compress_config=compress_config,\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    token=None,\n",
    "    padding_side=\"left\",\n",
    "    model_max_length=256,\n",
    "    use_fast=False,\n",
    "    cache_dir=\"../cache\",\n",
    ")\n",
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_kwargs = dict(\n",
    "    return_dict_in_generate=True,\n",
    "    max_length=None,\n",
    "    max_new_tokens=256,\n",
    "    output_scores=True,\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    "    use_cache=True,\n",
    ")\n",
    "\n",
    "\n",
    "generate_kwargs[\"do_sample\"] = False\n",
    "generate_kwargs[\"temperature\"] = None\n",
    "generate_kwargs[\"top_k\"] = None\n",
    "generate_kwargs[\"top_p\"] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 6])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/scratch/haokang/anaconda3/envs/test/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:389: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `None` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/usr/scratch/haokang/anaconda3/envs/test/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:394: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `None` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/usr/scratch/haokang/anaconda3/envs/test/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:404: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `None` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['mayonnaise. I love it on sandwiches, in salads, on burgers, in potato salad, on fries, in devilled eggs, on chicken, on fish, on pizza, on tacos, on baked potatoes, on hot dogs, on hamburgers, on sandwiches, on sandwiches, on sandwiches, on sandwiches, on sandwiches, on sandwiches, on sandwiches, on sandwiches, on sandwiches, on sandwiches, on sandwiches, on sandwiches, on sandwiches, on sandwiches, on sandwiches, on sandwiches, on sandwiches, on sandwiches, on sandwiches, on sandwiches, on sandwiches, on sandwiches, on sandwiches, on sandwiches, on sandwiches, on sandwiches, on sandwiches, on sandwiches, on sandwiches, on sandwiches, on sandwiches, on sandwiches, on sandwiches, on sandwiches, on sandwiches, on sandwiches, on sandwiches, on sandwiches, on sand', 'mayonnaise. I love it on sandwiches, in salads, on burgers, in potato salad, on fries, in devilled eggs, on chicken, on fish, on pizza, on tacos, on baked potatoes, on hot dogs, on hamburgers, on sandwiches, on sandwiches, on sandwiches, on sandwiches, on sandwiches, on sandwiches, on sandwiches, on sandwiches, on sandwiches, on sandwiches, on sandwiches, on sandwiches, on sandwiches, on sandwiches, on sandwiches, on sandwiches, on sandwiches, on sandwiches, on sandwiches, on sandwiches, on sandwiches, on sandwiches, on sandwiches, on sandwiches, on sandwiches, on sandwiches, on sandwiches, on sandwiches, on sandwiches, on sandwiches, on sandwiches, on sandwiches, on sandwiches, on sandwiches, on sandwiches, on sandwiches, on sandwiches, on sandwiches, on sand']\n"
     ]
    }
   ],
   "source": [
    "prompt = \"My favourite condiment is\"\n",
    "\n",
    "model_inputs = tokenizer([prompt,prompt], return_tensors=\"pt\").to(\"cuda\")\n",
    "print(model_inputs[\"input_ids\"].shape)\n",
    "generated_ids = model.generate(**model_inputs, **generate_kwargs)\n",
    "generations = tokenizer.batch_decode(\n",
    "    generated_ids.sequences[:, model_inputs.input_ids.shape[1] :],\n",
    "    skip_special_tokens=True,\n",
    ")\n",
    "print(generations)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
