{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import re\n",
    "import sys\n",
    "import torch\n",
    "import numpy as np\n",
    "import datasets\n",
    "import accelerate\n",
    "import transformers\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "from pathlib import Path\n",
    "from datasets import load_dataset\n",
    "from typing import Any, Callable, Dict, Sequence, cast\n",
    "from dataclasses import dataclass\n",
    "from dataclasses_json import DataClassJsonMixin\n",
    "\n",
    "from models import LlamaForCausalLMNew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IGNORE_INDEX = -100\n",
    "DEFAULT_PAD_TOKEN = \"[PAD]\"\n",
    "DEFAULT_EOS_TOKEN = \"</s>\"\n",
    "DEFAULT_BOS_TOKEN = \"<s>\"\n",
    "DEFAULT_UNK_TOKEN = \"<unk>\"\n",
    "\n",
    "MODEL_GENERATION_SPLIT = \"\\nQuestion: \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class EvaluationSample:\n",
    "    \"\"\"Wrapper around format evaluation sample.\"\"\"\n",
    "\n",
    "    question: str\n",
    "    generation: str\n",
    "    answer: str\n",
    "    list_from_pred: list[str]\n",
    "    list_from_answer: list[str]\n",
    "    pred: float\n",
    "    label: float\n",
    "    is_pred_true: bool\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class EvaluationMetrics(DataClassJsonMixin):\n",
    "    \"\"\"Wrapper around aggregated evaluation metrics.\"\"\"\n",
    "\n",
    "    accuracy: float\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class EvaluationResults(DataClassJsonMixin):\n",
    "    \"\"\"Wrapper around evaluation results\"\"\"\n",
    "\n",
    "    samples: list[EvaluationSample]\n",
    "    metrics: EvaluationMetrics\n",
    "\n",
    "\n",
    "def evaluate_pred_answer(pred_str, ans_str):\n",
    "    pattern = \"\\d*\\.?\\d+\"\n",
    "    pred_str, ans_str = pred_str.replace(\",\", \"\"), ans_str.replace(\",\", \"\")\n",
    "    pred_list = re.findall(pattern, pred_str)\n",
    "    gold_list = re.findall(pattern, ans_str)\n",
    "    if len(pred_list) >= 1:\n",
    "        pred = float(pred_list[-1])\n",
    "        gold = float(gold_list[-1])\n",
    "        is_pred_true = pred == gold\n",
    "    else:\n",
    "        is_pred_true = False\n",
    "        pred = None\n",
    "        gold = float(gold_list[-1])\n",
    "    return (\n",
    "        is_pred_true,\n",
    "        pred,\n",
    "        pred_list,\n",
    "        gold,\n",
    "        gold_list,\n",
    "    )\n",
    "\n",
    "\n",
    "def test_answer(pred_str, ans_str):\n",
    "    pattern = \"\\d*\\.?\\d+\"\n",
    "    pred = re.findall(pattern, pred_str)\n",
    "    if len(pred) >= 1:\n",
    "        print(\"#####\\n Pred string:\", pred_str, \"\\n pred_list\", pred)\n",
    "        pred = float(pred[-1].replace(\",\", \"\"))\n",
    "        gold = re.findall(pattern, ans_str)\n",
    "        print(\"\\n Gold_answer\", ans_str, \"\\n gold_list\", gold)\n",
    "        gold = float(gold[-1].replace(\",\", \"\"))\n",
    "        print(\"\\n result\", gold, pred, gold == pred)\n",
    "        return pred == gold\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "\n",
    "def parse_pred_ans(filename):\n",
    "    with open(filename) as fd:\n",
    "        lines = fd.readlines()\n",
    "    am, a = None, None\n",
    "    num_q, acc = 0, 0\n",
    "    current_mode = \"none\"\n",
    "    questions = []\n",
    "    ans_pred = []\n",
    "    ans_gold = []\n",
    "    am_others = []\n",
    "    for l in lines:\n",
    "        if l.startswith(\"Q: \"):\n",
    "            if am is not None and a is not None:\n",
    "                questions.append(q)\n",
    "                ans_pred.append(am)\n",
    "                ans_gold.append(a)\n",
    "                if test_answer(am, a):\n",
    "                    acc += 1\n",
    "            current_mode = \"q\"\n",
    "            q = l\n",
    "            num_q += 1\n",
    "        elif l.startswith(\"A_model:\"):\n",
    "            current_mode = \"am\"\n",
    "            am = l\n",
    "        elif l.startswith(\"A:\"):\n",
    "            current_mode = \"a\"\n",
    "            a = l\n",
    "        # TODO\n",
    "        elif current_mode == \"am\" and l.startswith(\"Question: \"):\n",
    "            current_mode = \"am_other\"\n",
    "            am_other = l\n",
    "        else:\n",
    "            if current_mode == \"q\":\n",
    "                q += l\n",
    "            elif current_mode == \"am\":\n",
    "                am += l\n",
    "            elif current_mode == \"a\":\n",
    "                a += l\n",
    "            elif current_mode == \"am_other\":\n",
    "                am_other += l\n",
    "            else:\n",
    "                raise ValueError(current_mode)\n",
    "\n",
    "    questions.append(q)\n",
    "    ans_pred.append(am)\n",
    "    ans_gold.append(a)\n",
    "    am_others.append(am_other)\n",
    "    if test_answer(am, a):\n",
    "        acc += 1\n",
    "    print(\"######\\n num_q %d correct %d ratio %.4f\" % (num_q, acc, float(acc / num_q)))\n",
    "    return questions, ans_pred, ans_gold\n",
    "\n",
    "\n",
    "def smart_tokenizer_and_embedding_resize(\n",
    "    special_tokens_dict: dict,\n",
    "    tokenizer: transformers.PreTrainedTokenizer,\n",
    "    model: transformers.PreTrainedModel,\n",
    "):\n",
    "    \"\"\"Resize tokenizer and embedding.\n",
    "\n",
    "    Note: This is the unoptimized version that may make your embedding size not be divisible by 64.\n",
    "    \"\"\"\n",
    "    num_new_tokens = tokenizer.add_special_tokens(special_tokens_dict)\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "    if num_new_tokens > 0:\n",
    "        input_embeddings = model.get_input_embeddings().weight.data\n",
    "        output_embeddings = model.get_output_embeddings().weight.data\n",
    "\n",
    "        input_embeddings_avg = input_embeddings[:-num_new_tokens].mean(\n",
    "            dim=0, keepdim=True\n",
    "        )\n",
    "        output_embeddings_avg = output_embeddings[:-num_new_tokens].mean(\n",
    "            dim=0, keepdim=True\n",
    "        )\n",
    "\n",
    "        input_embeddings[-num_new_tokens:] = input_embeddings_avg\n",
    "        output_embeddings[-num_new_tokens:] = output_embeddings_avg\n",
    "\n",
    "\n",
    "class StoppingCriteriaSub(transformers.StoppingCriteria):\n",
    "    def __init__(self, stops=[], encounters=1):\n",
    "        super().__init__()\n",
    "        self.stops = [stop.to(\"cuda\") for stop in stops]\n",
    "\n",
    "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor):\n",
    "        last_token = input_ids[0][-1]\n",
    "        for stop in self.stops:\n",
    "            if tokenizer.decode(stop) == tokenizer.decode(last_token):\n",
    "                return True\n",
    "        return False"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
