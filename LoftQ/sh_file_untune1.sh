python3 test_gsm8k.py --model_name_or_path meta-llama/Llama-2-7b-hf --batch_size 8 --device_map "cuda:1" --gpu 1 --compress_method outquantize_with_lrap --attention_number 32 --quantize_bit 4 --rank 0.10 --rankv 0.10 --loop 3 --left 0.01 > llama2_gsm8k_outquant4_0.01_lrap_0.1.txt
python3 test_gsm8k.py --model_name_or_path meta-llama/Llama-2-7b-hf --batch_size 8 --device_map "cuda:1" --gpu 1 --compress_method outquantize_with_lrap --attention_number 32 --quantize_bit 4 --rank 0.10 --rankv 0.10 --loop 3 --left 0.02 > llama2_gsm8k_outquant4_0.02_lrap_0.1.txt

python3 test_gsm8k.py --model_name_or_path meta-llama/Llama-2-7b-hf --batch_size 8 --device_map "cuda:1" --gpu 1 --compress_method outquantize_with_lrap --attention_number 32 --quantize_bit 4 --rank 0.05 --rankv 0.05 --loop 3 --left 0.01 > llama2_gsm8k_outquant4_0.01_lrap_0.05.txt
python3 test_gsm8k.py --model_name_or_path meta-llama/Llama-2-7b-hf --batch_size 8 --device_map "cuda:1" --gpu 1 --compress_method outquantize_with_lrap --attention_number 32 --quantize_bit 4 --rank 0.05 --rankv 0.05 --loop 3 --left 0.02 > llama2_gsm8k_outquant4_0.02_lrap_0.05.txt

python3 test_gsm8k.py --model_name_or_path meta-llama/Llama-2-7b-hf --batch_size 8 --device_map "cuda:1" --gpu 1 --compress_method outquantize_with_lrap --attention_number 32 --quantize_bit 6 --rank 0.10 --rankv 0.10 --loop 3 --left 0.01 > llama2_gsm8k_outquant6_0.01_lrap_0.1.txt
python3 test_gsm8k.py --model_name_or_path meta-llama/Llama-2-7b-hf --batch_size 8 --device_map "cuda:1" --gpu 1 --compress_method outquantize_with_lrap --attention_number 32 --quantize_bit 6 --rank 0.10 --rankv 0.10 --loop 3 --left 0.02 > llama2_gsm8k_outquant6_0.02_lrap_0.2.txt

python3 test_gsm8k.py --model_name_or_path meta-llama/Llama-2-7b-hf --batch_size 8 --device_map "cuda:1" --gpu 1 --compress_method outquantize_with_lrap --attention_number 32 --quantize_bit 6 --rank 0.05 --rankv 0.05 --loop 3 --left 0.01 > llama2_gsm8k_outquant6_0.01_lrap_0.05.txt
python3 test_gsm8k.py --model_name_or_path meta-llama/Llama-2-7b-hf --batch_size 8 --device_map "cuda:1" --gpu 1 --compress_method outquantize_with_lrap --attention_number 32 --quantize_bit 6 --rank 0.05 --rankv 0.05 --loop 3 --left 0.02 > llama2_gsm8k_outquant6_0.02_lrap_0.05.txt